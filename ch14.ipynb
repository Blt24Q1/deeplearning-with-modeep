{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 3s 37ms/step - loss: 0.2756 - accuracy: 0.9079 - val_loss: 0.2583 - val_accuracy: 0.9085\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2370 - accuracy: 0.9176 - val_loss: 0.2395 - val_accuracy: 0.9246\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2160 - accuracy: 0.9271 - val_loss: 0.2288 - val_accuracy: 0.9285\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2081 - accuracy: 0.9325 - val_loss: 0.2195 - val_accuracy: 0.9331\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2027 - accuracy: 0.9320 - val_loss: 0.2119 - val_accuracy: 0.9362\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1972 - accuracy: 0.9346 - val_loss: 0.2047 - val_accuracy: 0.9369\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1928 - accuracy: 0.9402 - val_loss: 0.1993 - val_accuracy: 0.9362\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1886 - accuracy: 0.9371 - val_loss: 0.1918 - val_accuracy: 0.9392\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1836 - accuracy: 0.9392 - val_loss: 0.1881 - val_accuracy: 0.9423\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1807 - accuracy: 0.9410 - val_loss: 0.1823 - val_accuracy: 0.9423\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1776 - accuracy: 0.9423 - val_loss: 0.1792 - val_accuracy: 0.9438\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1750 - accuracy: 0.9418 - val_loss: 0.1739 - val_accuracy: 0.9454\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1704 - accuracy: 0.9410 - val_loss: 0.1726 - val_accuracy: 0.9462\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1659 - accuracy: 0.9420 - val_loss: 0.1655 - val_accuracy: 0.9454\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1634 - accuracy: 0.9418 - val_loss: 0.1613 - val_accuracy: 0.9462\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1596 - accuracy: 0.9428 - val_loss: 0.1585 - val_accuracy: 0.9477\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1569 - accuracy: 0.9423 - val_loss: 0.1565 - val_accuracy: 0.9477\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1552 - accuracy: 0.9441 - val_loss: 0.1530 - val_accuracy: 0.9500\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1517 - accuracy: 0.9471 - val_loss: 0.1520 - val_accuracy: 0.9523\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1504 - accuracy: 0.9471 - val_loss: 0.1490 - val_accuracy: 0.9515\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1482 - accuracy: 0.9471 - val_loss: 0.1550 - val_accuracy: 0.9515\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1506 - accuracy: 0.9482 - val_loss: 0.1505 - val_accuracy: 0.9515\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1496 - accuracy: 0.9451 - val_loss: 0.1430 - val_accuracy: 0.9523\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1425 - accuracy: 0.9484 - val_loss: 0.1417 - val_accuracy: 0.9508\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1388 - accuracy: 0.9500 - val_loss: 0.1421 - val_accuracy: 0.9538\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1387 - accuracy: 0.9497 - val_loss: 0.1396 - val_accuracy: 0.9531\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1375 - accuracy: 0.9500 - val_loss: 0.1463 - val_accuracy: 0.9538\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1387 - accuracy: 0.9510 - val_loss: 0.1364 - val_accuracy: 0.9546\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9538 - val_loss: 0.1392 - val_accuracy: 0.9508\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1332 - accuracy: 0.9520 - val_loss: 0.1393 - val_accuracy: 0.9531\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1329 - accuracy: 0.9530 - val_loss: 0.1411 - val_accuracy: 0.9523\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9533 - val_loss: 0.1347 - val_accuracy: 0.9531\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1307 - accuracy: 0.9543 - val_loss: 0.1380 - val_accuracy: 0.9531\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1301 - accuracy: 0.9533 - val_loss: 0.1303 - val_accuracy: 0.9538\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1267 - accuracy: 0.9528 - val_loss: 0.1297 - val_accuracy: 0.9546\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1236 - accuracy: 0.9543 - val_loss: 0.1280 - val_accuracy: 0.9554\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1227 - accuracy: 0.9551 - val_loss: 0.1284 - val_accuracy: 0.9554\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1211 - accuracy: 0.9554 - val_loss: 0.1257 - val_accuracy: 0.9546\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1205 - accuracy: 0.9559 - val_loss: 0.1264 - val_accuracy: 0.9569\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1186 - accuracy: 0.9556 - val_loss: 0.1253 - val_accuracy: 0.9562\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1175 - accuracy: 0.9554 - val_loss: 0.1250 - val_accuracy: 0.9631\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1192 - accuracy: 0.9551 - val_loss: 0.1219 - val_accuracy: 0.9577\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1176 - accuracy: 0.9579 - val_loss: 0.1208 - val_accuracy: 0.9569\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1167 - accuracy: 0.9571 - val_loss: 0.1198 - val_accuracy: 0.9569\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9566 - val_loss: 0.1193 - val_accuracy: 0.9654\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1167 - accuracy: 0.9587 - val_loss: 0.1273 - val_accuracy: 0.9546\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1138 - accuracy: 0.9613 - val_loss: 0.1242 - val_accuracy: 0.9562\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1137 - accuracy: 0.9579 - val_loss: 0.1224 - val_accuracy: 0.9569\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1143 - accuracy: 0.9584 - val_loss: 0.1178 - val_accuracy: 0.9569\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1115 - accuracy: 0.9630 - val_loss: 0.1176 - val_accuracy: 0.9577\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 10ms/step - loss: 0.1375 - accuracy: 0.9423\n",
      "Test accuracy: 0.942307710647583\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # 학습 중간에 모델을 저장하는 콜백 \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.7954.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.9023.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.9162.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.9338.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.9400.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.9408.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.9338.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.9408.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.9369.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9415.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9408.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9423.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9408.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9408.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9415.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9438.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9431.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9462.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9438.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9438.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9469.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9446.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9469.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9469.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9477.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9477.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9515.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9492.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9500.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9500.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9523.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9538.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9515.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9500.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9523.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9562.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9554.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9562.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9562.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9569.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9585.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9577.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9569.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9592.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9600.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9631.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9608.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9608.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9631.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9600.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, \n",
    "                  callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 10ms/step - loss: 0.1186 - accuracy: 0.9569\n",
      "Test accuracy: 0.9569230675697327\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.115991</td>\n",
       "      <td>0.956120</td>\n",
       "      <td>0.118965</td>\n",
       "      <td>0.957692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115635</td>\n",
       "      <td>0.956633</td>\n",
       "      <td>0.113583</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112611</td>\n",
       "      <td>0.957660</td>\n",
       "      <td>0.112647</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110529</td>\n",
       "      <td>0.959456</td>\n",
       "      <td>0.116088</td>\n",
       "      <td>0.966923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.108743</td>\n",
       "      <td>0.957916</td>\n",
       "      <td>0.113860</td>\n",
       "      <td>0.966154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.992302</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.986923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.993585</td>\n",
       "      <td>0.068320</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.020562</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>0.077285</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.022608</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>0.077934</td>\n",
       "      <td>0.985385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.021131</td>\n",
       "      <td>0.993328</td>\n",
       "      <td>0.072012</td>\n",
       "      <td>0.986923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.115991  0.956120  0.118965      0.957692\n",
       "1     0.115635  0.956633  0.113583      0.960000\n",
       "2     0.112611  0.957660  0.112647      0.960000\n",
       "3     0.110529  0.959456  0.116088      0.966923\n",
       "4     0.108743  0.957916  0.113860      0.966154\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.025604  0.992302  0.071300      0.986923\n",
       "1996  0.022993  0.993585  0.068320      0.987692\n",
       "1997  0.020562  0.995124  0.077285      0.984615\n",
       "1998  0.022608  0.995124  0.077934      0.985385\n",
       "1999  0.021131  0.993328  0.072012      0.986923\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 5.2343 - accuracy: 0.2525 - val_loss: 3.3541 - val_accuracy: 0.2415\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.6663 - accuracy: 0.2525 - val_loss: 2.1294 - val_accuracy: 0.2415\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.6415 - accuracy: 0.2525 - val_loss: 1.1387 - val_accuracy: 0.2462\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.9006 - accuracy: 0.3254 - val_loss: 0.7399 - val_accuracy: 0.5254\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.6771 - accuracy: 0.6551 - val_loss: 0.6467 - val_accuracy: 0.7615\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.6270 - accuracy: 0.7980 - val_loss: 0.6263 - val_accuracy: 0.8492\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6109 - accuracy: 0.8635 - val_loss: 0.6164 - val_accuracy: 0.8885\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.6016 - accuracy: 0.8876 - val_loss: 0.6103 - val_accuracy: 0.8969\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5949 - accuracy: 0.9058 - val_loss: 0.6049 - val_accuracy: 0.9138\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5883 - accuracy: 0.9125 - val_loss: 0.5982 - val_accuracy: 0.9123\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5815 - accuracy: 0.9074 - val_loss: 0.5914 - val_accuracy: 0.9162\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.5737 - accuracy: 0.9145 - val_loss: 0.5859 - val_accuracy: 0.9185\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5687 - accuracy: 0.9220 - val_loss: 0.5826 - val_accuracy: 0.9246\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5649 - accuracy: 0.9287 - val_loss: 0.5779 - val_accuracy: 0.9238\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5607 - accuracy: 0.9243 - val_loss: 0.5732 - val_accuracy: 0.9254\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.5576 - accuracy: 0.9253 - val_loss: 0.5700 - val_accuracy: 0.9292\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5547 - accuracy: 0.9315 - val_loss: 0.5670 - val_accuracy: 0.9285\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.5515 - accuracy: 0.9276 - val_loss: 0.5629 - val_accuracy: 0.9277\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5489 - accuracy: 0.9307 - val_loss: 0.5605 - val_accuracy: 0.9300\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5456 - accuracy: 0.9320 - val_loss: 0.5565 - val_accuracy: 0.9323\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.5427 - accuracy: 0.9307 - val_loss: 0.5534 - val_accuracy: 0.9323\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.5398 - accuracy: 0.9351 - val_loss: 0.5503 - val_accuracy: 0.9346\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5369 - accuracy: 0.9343 - val_loss: 0.5471 - val_accuracy: 0.9315\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5343 - accuracy: 0.9335 - val_loss: 0.5445 - val_accuracy: 0.9331\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5316 - accuracy: 0.9356 - val_loss: 0.5411 - val_accuracy: 0.9323\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5287 - accuracy: 0.9343 - val_loss: 0.5382 - val_accuracy: 0.9346\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5261 - accuracy: 0.9353 - val_loss: 0.5354 - val_accuracy: 0.9354\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5234 - accuracy: 0.9361 - val_loss: 0.5324 - val_accuracy: 0.9346\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5208 - accuracy: 0.9346 - val_loss: 0.5295 - val_accuracy: 0.9346\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5183 - accuracy: 0.9379 - val_loss: 0.5267 - val_accuracy: 0.9338\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5157 - accuracy: 0.9374 - val_loss: 0.5238 - val_accuracy: 0.9369\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5131 - accuracy: 0.9366 - val_loss: 0.5212 - val_accuracy: 0.9369\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5106 - accuracy: 0.9400 - val_loss: 0.5182 - val_accuracy: 0.9377\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5080 - accuracy: 0.9364 - val_loss: 0.5156 - val_accuracy: 0.9385\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5057 - accuracy: 0.9412 - val_loss: 0.5130 - val_accuracy: 0.9400\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5034 - accuracy: 0.9379 - val_loss: 0.5103 - val_accuracy: 0.9377\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5006 - accuracy: 0.9407 - val_loss: 0.5077 - val_accuracy: 0.9392\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4983 - accuracy: 0.9402 - val_loss: 0.5051 - val_accuracy: 0.9392\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4957 - accuracy: 0.9412 - val_loss: 0.5025 - val_accuracy: 0.9377\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4936 - accuracy: 0.9407 - val_loss: 0.5003 - val_accuracy: 0.9400\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4909 - accuracy: 0.9423 - val_loss: 0.4974 - val_accuracy: 0.9392\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4887 - accuracy: 0.9415 - val_loss: 0.4952 - val_accuracy: 0.9431\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4863 - accuracy: 0.9430 - val_loss: 0.4929 - val_accuracy: 0.9431\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4840 - accuracy: 0.9430 - val_loss: 0.4907 - val_accuracy: 0.9431\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.4817 - accuracy: 0.9425 - val_loss: 0.4879 - val_accuracy: 0.9408\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4796 - accuracy: 0.9412 - val_loss: 0.4865 - val_accuracy: 0.9415\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.4781 - accuracy: 0.9443 - val_loss: 0.4834 - val_accuracy: 0.9438\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4760 - accuracy: 0.9394 - val_loss: 0.4811 - val_accuracy: 0.9438\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4739 - accuracy: 0.9446 - val_loss: 0.4805 - val_accuracy: 0.9431\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4713 - accuracy: 0.9405 - val_loss: 0.4766 - val_accuracy: 0.9446\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4684 - accuracy: 0.9446 - val_loss: 0.4746 - val_accuracy: 0.9454\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4661 - accuracy: 0.9456 - val_loss: 0.4721 - val_accuracy: 0.9454\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4648 - accuracy: 0.9412 - val_loss: 0.4707 - val_accuracy: 0.9454\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.4619 - accuracy: 0.9451 - val_loss: 0.4679 - val_accuracy: 0.9454\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4597 - accuracy: 0.9443 - val_loss: 0.4658 - val_accuracy: 0.9462\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4575 - accuracy: 0.9453 - val_loss: 0.4637 - val_accuracy: 0.9462\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4555 - accuracy: 0.9459 - val_loss: 0.4615 - val_accuracy: 0.9454\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4534 - accuracy: 0.9459 - val_loss: 0.4593 - val_accuracy: 0.9454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4513 - accuracy: 0.9448 - val_loss: 0.4573 - val_accuracy: 0.9462\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4494 - accuracy: 0.9466 - val_loss: 0.4549 - val_accuracy: 0.9454\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4476 - accuracy: 0.9446 - val_loss: 0.4535 - val_accuracy: 0.9477\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4453 - accuracy: 0.9464 - val_loss: 0.4514 - val_accuracy: 0.9469\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4430 - accuracy: 0.9448 - val_loss: 0.4488 - val_accuracy: 0.9454\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4409 - accuracy: 0.9456 - val_loss: 0.4470 - val_accuracy: 0.9469\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4390 - accuracy: 0.9443 - val_loss: 0.4452 - val_accuracy: 0.9469\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4373 - accuracy: 0.9469 - val_loss: 0.4430 - val_accuracy: 0.9469\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4356 - accuracy: 0.9479 - val_loss: 0.4416 - val_accuracy: 0.9485\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4328 - accuracy: 0.9466 - val_loss: 0.4384 - val_accuracy: 0.9462\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4308 - accuracy: 0.9469 - val_loss: 0.4376 - val_accuracy: 0.9492\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4292 - accuracy: 0.9477 - val_loss: 0.4346 - val_accuracy: 0.9462\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.4269 - accuracy: 0.9477 - val_loss: 0.4325 - val_accuracy: 0.9462\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.4250 - accuracy: 0.9474 - val_loss: 0.4315 - val_accuracy: 0.9492\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4229 - accuracy: 0.9482 - val_loss: 0.4286 - val_accuracy: 0.9469\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4211 - accuracy: 0.9477 - val_loss: 0.4273 - val_accuracy: 0.9485\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4193 - accuracy: 0.9492 - val_loss: 0.4250 - val_accuracy: 0.9477\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4171 - accuracy: 0.9477 - val_loss: 0.4230 - val_accuracy: 0.9485\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4155 - accuracy: 0.9497 - val_loss: 0.4220 - val_accuracy: 0.9500\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4146 - accuracy: 0.9482 - val_loss: 0.4196 - val_accuracy: 0.9500\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4138 - accuracy: 0.9479 - val_loss: 0.4188 - val_accuracy: 0.9508\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4101 - accuracy: 0.9494 - val_loss: 0.4160 - val_accuracy: 0.9500\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4079 - accuracy: 0.9497 - val_loss: 0.4140 - val_accuracy: 0.9515\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4058 - accuracy: 0.9487 - val_loss: 0.4118 - val_accuracy: 0.9500\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4044 - accuracy: 0.9489 - val_loss: 0.4099 - val_accuracy: 0.9500\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4019 - accuracy: 0.9494 - val_loss: 0.4090 - val_accuracy: 0.9515\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4016 - accuracy: 0.9497 - val_loss: 0.4071 - val_accuracy: 0.9515\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3989 - accuracy: 0.9484 - val_loss: 0.4052 - val_accuracy: 0.9515\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3961 - accuracy: 0.9512 - val_loss: 0.4025 - val_accuracy: 0.9523\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3941 - accuracy: 0.9497 - val_loss: 0.4009 - val_accuracy: 0.9515\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3925 - accuracy: 0.9505 - val_loss: 0.4003 - val_accuracy: 0.9515\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3912 - accuracy: 0.9497 - val_loss: 0.3969 - val_accuracy: 0.9515\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3882 - accuracy: 0.9520 - val_loss: 0.3963 - val_accuracy: 0.9515\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3868 - accuracy: 0.9520 - val_loss: 0.3930 - val_accuracy: 0.9515\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3853 - accuracy: 0.9500 - val_loss: 0.3932 - val_accuracy: 0.9515\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3830 - accuracy: 0.9515 - val_loss: 0.3897 - val_accuracy: 0.9531\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3805 - accuracy: 0.9520 - val_loss: 0.3881 - val_accuracy: 0.9531\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3789 - accuracy: 0.9507 - val_loss: 0.3865 - val_accuracy: 0.9523\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3777 - accuracy: 0.9536 - val_loss: 0.3836 - val_accuracy: 0.9546\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3752 - accuracy: 0.9520 - val_loss: 0.3824 - val_accuracy: 0.9531\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3727 - accuracy: 0.9525 - val_loss: 0.3796 - val_accuracy: 0.9577\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3704 - accuracy: 0.9541 - val_loss: 0.3774 - val_accuracy: 0.9569\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3684 - accuracy: 0.9551 - val_loss: 0.3756 - val_accuracy: 0.9569\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3662 - accuracy: 0.9556 - val_loss: 0.3725 - val_accuracy: 0.9585\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3629 - accuracy: 0.9579 - val_loss: 0.3692 - val_accuracy: 0.9631\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3592 - accuracy: 0.9641 - val_loss: 0.3700 - val_accuracy: 0.9577\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3569 - accuracy: 0.9630 - val_loss: 0.3631 - val_accuracy: 0.9638\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3535 - accuracy: 0.9661 - val_loss: 0.3616 - val_accuracy: 0.9638\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3510 - accuracy: 0.9648 - val_loss: 0.3640 - val_accuracy: 0.9608\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3519 - accuracy: 0.9618 - val_loss: 0.3584 - val_accuracy: 0.9662\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3508 - accuracy: 0.9643 - val_loss: 0.3575 - val_accuracy: 0.9631\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.3472 - accuracy: 0.9641 - val_loss: 0.3531 - val_accuracy: 0.9669\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3452 - accuracy: 0.9646 - val_loss: 0.3542 - val_accuracy: 0.9631\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3413 - accuracy: 0.9656 - val_loss: 0.3523 - val_accuracy: 0.9638\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3408 - accuracy: 0.9656 - val_loss: 0.3478 - val_accuracy: 0.9669\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3376 - accuracy: 0.9674 - val_loss: 0.3447 - val_accuracy: 0.9669\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3346 - accuracy: 0.9690 - val_loss: 0.3432 - val_accuracy: 0.9669\n",
      "Epoch 116/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3327 - accuracy: 0.9695 - val_loss: 0.3436 - val_accuracy: 0.9654\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3320 - accuracy: 0.9697 - val_loss: 0.3391 - val_accuracy: 0.9700\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3321 - accuracy: 0.9656 - val_loss: 0.3388 - val_accuracy: 0.9662\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3300 - accuracy: 0.9664 - val_loss: 0.3348 - val_accuracy: 0.9685\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3253 - accuracy: 0.9713 - val_loss: 0.3343 - val_accuracy: 0.9677\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3233 - accuracy: 0.9702 - val_loss: 0.3326 - val_accuracy: 0.9677\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3221 - accuracy: 0.9710 - val_loss: 0.3341 - val_accuracy: 0.9669\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3208 - accuracy: 0.9682 - val_loss: 0.3281 - val_accuracy: 0.9685\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3175 - accuracy: 0.9731 - val_loss: 0.3255 - val_accuracy: 0.9700\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3152 - accuracy: 0.9731 - val_loss: 0.3237 - val_accuracy: 0.9715\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3133 - accuracy: 0.9725 - val_loss: 0.3220 - val_accuracy: 0.9708\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.3117 - accuracy: 0.9738 - val_loss: 0.3205 - val_accuracy: 0.9700\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3101 - accuracy: 0.9743 - val_loss: 0.3181 - val_accuracy: 0.9715\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3078 - accuracy: 0.9746 - val_loss: 0.3178 - val_accuracy: 0.9708\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3069 - accuracy: 0.9731 - val_loss: 0.3142 - val_accuracy: 0.9700\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3047 - accuracy: 0.9743 - val_loss: 0.3124 - val_accuracy: 0.9708\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3029 - accuracy: 0.9756 - val_loss: 0.3123 - val_accuracy: 0.9700\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3001 - accuracy: 0.9779 - val_loss: 0.3098 - val_accuracy: 0.9692\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3014 - accuracy: 0.9733 - val_loss: 0.3080 - val_accuracy: 0.9692\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3001 - accuracy: 0.9746 - val_loss: 0.3065 - val_accuracy: 0.9708\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2967 - accuracy: 0.9749 - val_loss: 0.3035 - val_accuracy: 0.9700\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2966 - accuracy: 0.9746 - val_loss: 0.3022 - val_accuracy: 0.9723\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2929 - accuracy: 0.9769 - val_loss: 0.3000 - val_accuracy: 0.9715\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2909 - accuracy: 0.9784 - val_loss: 0.3000 - val_accuracy: 0.9708\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2900 - accuracy: 0.9764 - val_loss: 0.3005 - val_accuracy: 0.9700\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2900 - accuracy: 0.9764 - val_loss: 0.3034 - val_accuracy: 0.9677\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2885 - accuracy: 0.9774 - val_loss: 0.2975 - val_accuracy: 0.9708\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2862 - accuracy: 0.9772 - val_loss: 0.2910 - val_accuracy: 0.9715\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2827 - accuracy: 0.9784 - val_loss: 0.2895 - val_accuracy: 0.9723\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2820 - accuracy: 0.9772 - val_loss: 0.2880 - val_accuracy: 0.9723\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2825 - accuracy: 0.9756 - val_loss: 0.2865 - val_accuracy: 0.9715\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2808 - accuracy: 0.9777 - val_loss: 0.2854 - val_accuracy: 0.9708\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2777 - accuracy: 0.9784 - val_loss: 0.2869 - val_accuracy: 0.9700\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2766 - accuracy: 0.9779 - val_loss: 0.2818 - val_accuracy: 0.9731\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2745 - accuracy: 0.9792 - val_loss: 0.2825 - val_accuracy: 0.9723\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2752 - accuracy: 0.9779 - val_loss: 0.2850 - val_accuracy: 0.9715\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2732 - accuracy: 0.9782 - val_loss: 0.2832 - val_accuracy: 0.9723\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2718 - accuracy: 0.9779 - val_loss: 0.2841 - val_accuracy: 0.9700\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2723 - accuracy: 0.9779 - val_loss: 0.2772 - val_accuracy: 0.9723\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2703 - accuracy: 0.9795 - val_loss: 0.2780 - val_accuracy: 0.9723\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2719 - accuracy: 0.9736 - val_loss: 0.2755 - val_accuracy: 0.9715\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2657 - accuracy: 0.9792 - val_loss: 0.2711 - val_accuracy: 0.9754\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2635 - accuracy: 0.9792 - val_loss: 0.2729 - val_accuracy: 0.9723\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2623 - accuracy: 0.9795 - val_loss: 0.2720 - val_accuracy: 0.9731\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2642 - accuracy: 0.9782 - val_loss: 0.2681 - val_accuracy: 0.9738\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2616 - accuracy: 0.9777 - val_loss: 0.2663 - val_accuracy: 0.9723\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2603 - accuracy: 0.9797 - val_loss: 0.2639 - val_accuracy: 0.9746\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2592 - accuracy: 0.9787 - val_loss: 0.2629 - val_accuracy: 0.9754\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2560 - accuracy: 0.9790 - val_loss: 0.2654 - val_accuracy: 0.9738\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2556 - accuracy: 0.9795 - val_loss: 0.2635 - val_accuracy: 0.9731\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2529 - accuracy: 0.9808 - val_loss: 0.2596 - val_accuracy: 0.9738\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2514 - accuracy: 0.9808 - val_loss: 0.2613 - val_accuracy: 0.9723\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2518 - accuracy: 0.9805 - val_loss: 0.2565 - val_accuracy: 0.9762\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2498 - accuracy: 0.9800 - val_loss: 0.2553 - val_accuracy: 0.9769\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2500 - accuracy: 0.9797 - val_loss: 0.2546 - val_accuracy: 0.9746\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2461 - accuracy: 0.9808 - val_loss: 0.2536 - val_accuracy: 0.9731\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2453 - accuracy: 0.9815 - val_loss: 0.2516 - val_accuracy: 0.9754\n",
      "Epoch 173/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2435 - accuracy: 0.9808 - val_loss: 0.2505 - val_accuracy: 0.9746\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2447 - accuracy: 0.9802 - val_loss: 0.2576 - val_accuracy: 0.9731\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2431 - accuracy: 0.9802 - val_loss: 0.2493 - val_accuracy: 0.9754\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2399 - accuracy: 0.9810 - val_loss: 0.2473 - val_accuracy: 0.9746\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2397 - accuracy: 0.9820 - val_loss: 0.2460 - val_accuracy: 0.9746\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2390 - accuracy: 0.9810 - val_loss: 0.2495 - val_accuracy: 0.9708\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2404 - accuracy: 0.9784 - val_loss: 0.2434 - val_accuracy: 0.9762\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2361 - accuracy: 0.9818 - val_loss: 0.2454 - val_accuracy: 0.9746\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2378 - accuracy: 0.9800 - val_loss: 0.2424 - val_accuracy: 0.9746\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2336 - accuracy: 0.9820 - val_loss: 0.2406 - val_accuracy: 0.9754\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2322 - accuracy: 0.9815 - val_loss: 0.2402 - val_accuracy: 0.9777\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2323 - accuracy: 0.9828 - val_loss: 0.2413 - val_accuracy: 0.9754\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2343 - accuracy: 0.9808 - val_loss: 0.2366 - val_accuracy: 0.9792\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2314 - accuracy: 0.9808 - val_loss: 0.2356 - val_accuracy: 0.9754\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2282 - accuracy: 0.9833 - val_loss: 0.2356 - val_accuracy: 0.9754\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2279 - accuracy: 0.9820 - val_loss: 0.2339 - val_accuracy: 0.9792\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2261 - accuracy: 0.9815 - val_loss: 0.2333 - val_accuracy: 0.9746\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2303 - accuracy: 0.9797 - val_loss: 0.2356 - val_accuracy: 0.9762\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2274 - accuracy: 0.9808 - val_loss: 0.2391 - val_accuracy: 0.9723\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2260 - accuracy: 0.9818 - val_loss: 0.2309 - val_accuracy: 0.9785\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2222 - accuracy: 0.9810 - val_loss: 0.2287 - val_accuracy: 0.9754\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2218 - accuracy: 0.9823 - val_loss: 0.2275 - val_accuracy: 0.9785\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2203 - accuracy: 0.9828 - val_loss: 0.2266 - val_accuracy: 0.9785\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2191 - accuracy: 0.9823 - val_loss: 0.2267 - val_accuracy: 0.9792\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2191 - accuracy: 0.9823 - val_loss: 0.2298 - val_accuracy: 0.9769\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2185 - accuracy: 0.9810 - val_loss: 0.2299 - val_accuracy: 0.9769\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2210 - accuracy: 0.9805 - val_loss: 0.2223 - val_accuracy: 0.9785\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2158 - accuracy: 0.9818 - val_loss: 0.2216 - val_accuracy: 0.9762\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2160 - accuracy: 0.9823 - val_loss: 0.2215 - val_accuracy: 0.9754\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2159 - accuracy: 0.9831 - val_loss: 0.2211 - val_accuracy: 0.9754\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2145 - accuracy: 0.9808 - val_loss: 0.2197 - val_accuracy: 0.9792\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2119 - accuracy: 0.9831 - val_loss: 0.2196 - val_accuracy: 0.9800\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2108 - accuracy: 0.9823 - val_loss: 0.2175 - val_accuracy: 0.9785\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2107 - accuracy: 0.9831 - val_loss: 0.2159 - val_accuracy: 0.9808\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2104 - accuracy: 0.9820 - val_loss: 0.2148 - val_accuracy: 0.9808\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2087 - accuracy: 0.9833 - val_loss: 0.2137 - val_accuracy: 0.9792\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2074 - accuracy: 0.9836 - val_loss: 0.2129 - val_accuracy: 0.9800\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2060 - accuracy: 0.9838 - val_loss: 0.2121 - val_accuracy: 0.9808\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2051 - accuracy: 0.9836 - val_loss: 0.2110 - val_accuracy: 0.9800\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2045 - accuracy: 0.9838 - val_loss: 0.2102 - val_accuracy: 0.9815\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2038 - accuracy: 0.9849 - val_loss: 0.2110 - val_accuracy: 0.9815\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2058 - accuracy: 0.9823 - val_loss: 0.2084 - val_accuracy: 0.9785\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2027 - accuracy: 0.9820 - val_loss: 0.2076 - val_accuracy: 0.9808\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2004 - accuracy: 0.9843 - val_loss: 0.2068 - val_accuracy: 0.9785\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2018 - accuracy: 0.9823 - val_loss: 0.2059 - val_accuracy: 0.9815\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1996 - accuracy: 0.9841 - val_loss: 0.2048 - val_accuracy: 0.9808\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1981 - accuracy: 0.9843 - val_loss: 0.2041 - val_accuracy: 0.9808\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1973 - accuracy: 0.9841 - val_loss: 0.2031 - val_accuracy: 0.9808\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1983 - accuracy: 0.9833 - val_loss: 0.2026 - val_accuracy: 0.9808\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1958 - accuracy: 0.9826 - val_loss: 0.2017 - val_accuracy: 0.9808\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1965 - accuracy: 0.9823 - val_loss: 0.2015 - val_accuracy: 0.9823\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1944 - accuracy: 0.9843 - val_loss: 0.2015 - val_accuracy: 0.9815\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1935 - accuracy: 0.9836 - val_loss: 0.2008 - val_accuracy: 0.9823\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1963 - accuracy: 0.9813 - val_loss: 0.1985 - val_accuracy: 0.9808\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1931 - accuracy: 0.9838 - val_loss: 0.1973 - val_accuracy: 0.9800\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1912 - accuracy: 0.9856 - val_loss: 0.1973 - val_accuracy: 0.9785\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1919 - accuracy: 0.9838 - val_loss: 0.1959 - val_accuracy: 0.9800\n",
      "Epoch 230/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1891 - accuracy: 0.9851 - val_loss: 0.1958 - val_accuracy: 0.9823\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1897 - accuracy: 0.9836 - val_loss: 0.1946 - val_accuracy: 0.9823\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1878 - accuracy: 0.9846 - val_loss: 0.1937 - val_accuracy: 0.9800\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1877 - accuracy: 0.9846 - val_loss: 0.1926 - val_accuracy: 0.9800\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1880 - accuracy: 0.9841 - val_loss: 0.1931 - val_accuracy: 0.9777\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1871 - accuracy: 0.9841 - val_loss: 0.1909 - val_accuracy: 0.9808\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1871 - accuracy: 0.9823 - val_loss: 0.1914 - val_accuracy: 0.9777\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1869 - accuracy: 0.9826 - val_loss: 0.1914 - val_accuracy: 0.9785\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1834 - accuracy: 0.9859 - val_loss: 0.1885 - val_accuracy: 0.9800\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1837 - accuracy: 0.9841 - val_loss: 0.1970 - val_accuracy: 0.9769\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1831 - accuracy: 0.9836 - val_loss: 0.1889 - val_accuracy: 0.9831\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1818 - accuracy: 0.9849 - val_loss: 0.1869 - val_accuracy: 0.9808\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1812 - accuracy: 0.9831 - val_loss: 0.1864 - val_accuracy: 0.9800\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1815 - accuracy: 0.9833 - val_loss: 0.1860 - val_accuracy: 0.9823\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1801 - accuracy: 0.9836 - val_loss: 0.1850 - val_accuracy: 0.9792\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1804 - accuracy: 0.9831 - val_loss: 0.1845 - val_accuracy: 0.9754\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1795 - accuracy: 0.9818 - val_loss: 0.1844 - val_accuracy: 0.9762\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1774 - accuracy: 0.9846 - val_loss: 0.1819 - val_accuracy: 0.9800\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1772 - accuracy: 0.9856 - val_loss: 0.1819 - val_accuracy: 0.9823\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1774 - accuracy: 0.9836 - val_loss: 0.1909 - val_accuracy: 0.9762\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1776 - accuracy: 0.9836 - val_loss: 0.1884 - val_accuracy: 0.9785\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1774 - accuracy: 0.9823 - val_loss: 0.1861 - val_accuracy: 0.9785\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1758 - accuracy: 0.9841 - val_loss: 0.1789 - val_accuracy: 0.9823\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1737 - accuracy: 0.9851 - val_loss: 0.1788 - val_accuracy: 0.9808\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1745 - accuracy: 0.9836 - val_loss: 0.1819 - val_accuracy: 0.9762\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1754 - accuracy: 0.9828 - val_loss: 0.1779 - val_accuracy: 0.9777\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1724 - accuracy: 0.9846 - val_loss: 0.1839 - val_accuracy: 0.9785\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1762 - accuracy: 0.9813 - val_loss: 0.1979 - val_accuracy: 0.9685\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1783 - accuracy: 0.9810 - val_loss: 0.1783 - val_accuracy: 0.9800\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1734 - accuracy: 0.9820 - val_loss: 0.1752 - val_accuracy: 0.9831\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1724 - accuracy: 0.9828 - val_loss: 0.1775 - val_accuracy: 0.9762\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1723 - accuracy: 0.9838 - val_loss: 0.1755 - val_accuracy: 0.9762\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1729 - accuracy: 0.9808 - val_loss: 0.1733 - val_accuracy: 0.9823\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1703 - accuracy: 0.9831 - val_loss: 0.1733 - val_accuracy: 0.9823\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1651 - accuracy: 0.9861 - val_loss: 0.1706 - val_accuracy: 0.9800\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1648 - accuracy: 0.9849 - val_loss: 0.1699 - val_accuracy: 0.9800\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1649 - accuracy: 0.9851 - val_loss: 0.1714 - val_accuracy: 0.9785\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1670 - accuracy: 0.9838 - val_loss: 0.1690 - val_accuracy: 0.9823\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1628 - accuracy: 0.9861 - val_loss: 0.1682 - val_accuracy: 0.9800\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1641 - accuracy: 0.9851 - val_loss: 0.1685 - val_accuracy: 0.9823\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1629 - accuracy: 0.9828 - val_loss: 0.1667 - val_accuracy: 0.9800\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1610 - accuracy: 0.9856 - val_loss: 0.1665 - val_accuracy: 0.9792\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1601 - accuracy: 0.9864 - val_loss: 0.1683 - val_accuracy: 0.9838\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1605 - accuracy: 0.9843 - val_loss: 0.1666 - val_accuracy: 0.9823\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1605 - accuracy: 0.9851 - val_loss: 0.1661 - val_accuracy: 0.9831\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1587 - accuracy: 0.9851 - val_loss: 0.1692 - val_accuracy: 0.9815\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1621 - accuracy: 0.9831 - val_loss: 0.1672 - val_accuracy: 0.9815\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1593 - accuracy: 0.9833 - val_loss: 0.1627 - val_accuracy: 0.9792\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1596 - accuracy: 0.9841 - val_loss: 0.1704 - val_accuracy: 0.9723\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1618 - accuracy: 0.9818 - val_loss: 0.1631 - val_accuracy: 0.9785\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1570 - accuracy: 0.9849 - val_loss: 0.1614 - val_accuracy: 0.9823\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1562 - accuracy: 0.9833 - val_loss: 0.1607 - val_accuracy: 0.9800\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1543 - accuracy: 0.9856 - val_loss: 0.1605 - val_accuracy: 0.9831\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1537 - accuracy: 0.9869 - val_loss: 0.1622 - val_accuracy: 0.9838\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1531 - accuracy: 0.9854 - val_loss: 0.1600 - val_accuracy: 0.9823\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1554 - accuracy: 0.9836 - val_loss: 0.1691 - val_accuracy: 0.9777\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1591 - accuracy: 0.9818 - val_loss: 0.1574 - val_accuracy: 0.9815\n",
      "Epoch 287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1519 - accuracy: 0.9843 - val_loss: 0.1567 - val_accuracy: 0.9815\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1521 - accuracy: 0.9851 - val_loss: 0.1567 - val_accuracy: 0.9823\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1508 - accuracy: 0.9846 - val_loss: 0.1566 - val_accuracy: 0.9823\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1511 - accuracy: 0.9838 - val_loss: 0.1568 - val_accuracy: 0.9808\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1508 - accuracy: 0.9856 - val_loss: 0.1548 - val_accuracy: 0.9808\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1492 - accuracy: 0.9859 - val_loss: 0.1558 - val_accuracy: 0.9823\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1489 - accuracy: 0.9861 - val_loss: 0.1568 - val_accuracy: 0.9823\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1485 - accuracy: 0.9854 - val_loss: 0.1575 - val_accuracy: 0.9823\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1480 - accuracy: 0.9854 - val_loss: 0.1570 - val_accuracy: 0.9823\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1492 - accuracy: 0.9841 - val_loss: 0.1597 - val_accuracy: 0.9808\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1502 - accuracy: 0.9836 - val_loss: 0.1552 - val_accuracy: 0.9823\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1457 - accuracy: 0.9861 - val_loss: 0.1511 - val_accuracy: 0.9815\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1458 - accuracy: 0.9856 - val_loss: 0.1523 - val_accuracy: 0.9815\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1456 - accuracy: 0.9856 - val_loss: 0.1502 - val_accuracy: 0.9831\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1448 - accuracy: 0.9851 - val_loss: 0.1496 - val_accuracy: 0.9815\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1443 - accuracy: 0.9864 - val_loss: 0.1566 - val_accuracy: 0.9738\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1466 - accuracy: 0.9851 - val_loss: 0.1493 - val_accuracy: 0.9800\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1441 - accuracy: 0.9846 - val_loss: 0.1486 - val_accuracy: 0.9823\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1422 - accuracy: 0.9859 - val_loss: 0.1477 - val_accuracy: 0.9808\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1424 - accuracy: 0.9856 - val_loss: 0.1471 - val_accuracy: 0.9815\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1413 - accuracy: 0.9851 - val_loss: 0.1467 - val_accuracy: 0.9808\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1418 - accuracy: 0.9846 - val_loss: 0.1460 - val_accuracy: 0.9815\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1408 - accuracy: 0.9846 - val_loss: 0.1456 - val_accuracy: 0.9815\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1404 - accuracy: 0.9854 - val_loss: 0.1452 - val_accuracy: 0.9815\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1413 - accuracy: 0.9849 - val_loss: 0.1475 - val_accuracy: 0.9800\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1405 - accuracy: 0.9854 - val_loss: 0.1464 - val_accuracy: 0.9800\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1400 - accuracy: 0.9843 - val_loss: 0.1448 - val_accuracy: 0.9815\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1382 - accuracy: 0.9856 - val_loss: 0.1434 - val_accuracy: 0.9815\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1373 - accuracy: 0.9864 - val_loss: 0.1524 - val_accuracy: 0.9800\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1398 - accuracy: 0.9849 - val_loss: 0.1487 - val_accuracy: 0.9823\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1394 - accuracy: 0.9831 - val_loss: 0.1419 - val_accuracy: 0.9823\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1371 - accuracy: 0.9849 - val_loss: 0.1412 - val_accuracy: 0.9823\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1391 - accuracy: 0.9843 - val_loss: 0.1440 - val_accuracy: 0.9800\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1385 - accuracy: 0.9851 - val_loss: 0.1417 - val_accuracy: 0.9815\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1399 - accuracy: 0.9833 - val_loss: 0.1398 - val_accuracy: 0.9815\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1358 - accuracy: 0.9859 - val_loss: 0.1477 - val_accuracy: 0.9815\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1385 - accuracy: 0.9831 - val_loss: 0.1437 - val_accuracy: 0.9815\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1353 - accuracy: 0.9851 - val_loss: 0.1434 - val_accuracy: 0.9815\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1373 - accuracy: 0.9846 - val_loss: 0.1590 - val_accuracy: 0.9738\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1396 - accuracy: 0.9826 - val_loss: 0.1440 - val_accuracy: 0.9815\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1360 - accuracy: 0.9836 - val_loss: 0.1373 - val_accuracy: 0.9823\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9867 - val_loss: 0.1372 - val_accuracy: 0.9815\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1310 - accuracy: 0.9856 - val_loss: 0.1380 - val_accuracy: 0.9815\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1303 - accuracy: 0.9859 - val_loss: 0.1365 - val_accuracy: 0.9823\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1302 - accuracy: 0.9854 - val_loss: 0.1365 - val_accuracy: 0.9815\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1431 - accuracy: 0.9805 - val_loss: 0.1430 - val_accuracy: 0.9815\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1345 - accuracy: 0.9838 - val_loss: 0.1432 - val_accuracy: 0.9815\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1357 - accuracy: 0.9836 - val_loss: 0.1488 - val_accuracy: 0.9762\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9818 - val_loss: 0.1613 - val_accuracy: 0.9685\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1428 - accuracy: 0.9779 - val_loss: 0.1374 - val_accuracy: 0.9800\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1295 - accuracy: 0.9867 - val_loss: 0.1338 - val_accuracy: 0.9815\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1288 - accuracy: 0.9849 - val_loss: 0.1332 - val_accuracy: 0.9815\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1277 - accuracy: 0.9864 - val_loss: 0.1337 - val_accuracy: 0.9808\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1272 - accuracy: 0.9849 - val_loss: 0.1330 - val_accuracy: 0.9808\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1256 - accuracy: 0.9861 - val_loss: 0.1323 - val_accuracy: 0.9815\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1256 - accuracy: 0.9849 - val_loss: 0.1316 - val_accuracy: 0.9815\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1264 - accuracy: 0.9856 - val_loss: 0.1344 - val_accuracy: 0.9808\n",
      "Epoch 344/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1258 - accuracy: 0.9856 - val_loss: 0.1373 - val_accuracy: 0.9823\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1255 - accuracy: 0.9854 - val_loss: 0.1322 - val_accuracy: 0.9823\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1249 - accuracy: 0.9856 - val_loss: 0.1296 - val_accuracy: 0.9823\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1248 - accuracy: 0.9859 - val_loss: 0.1290 - val_accuracy: 0.9815\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1246 - accuracy: 0.9851 - val_loss: 0.1289 - val_accuracy: 0.9823\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1225 - accuracy: 0.9861 - val_loss: 0.1302 - val_accuracy: 0.9823\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1220 - accuracy: 0.9867 - val_loss: 0.1279 - val_accuracy: 0.9823\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1234 - accuracy: 0.9851 - val_loss: 0.1298 - val_accuracy: 0.9823\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1237 - accuracy: 0.9841 - val_loss: 0.1274 - val_accuracy: 0.9815\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9851 - val_loss: 0.1290 - val_accuracy: 0.9815\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1255 - accuracy: 0.9833 - val_loss: 0.1356 - val_accuracy: 0.9815\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1280 - accuracy: 0.9828 - val_loss: 0.1424 - val_accuracy: 0.9754\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1247 - accuracy: 0.9838 - val_loss: 0.1341 - val_accuracy: 0.9808\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1219 - accuracy: 0.9861 - val_loss: 0.1251 - val_accuracy: 0.9831\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1200 - accuracy: 0.9877 - val_loss: 0.1253 - val_accuracy: 0.9831\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1207 - accuracy: 0.9859 - val_loss: 0.1260 - val_accuracy: 0.9831\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9833 - val_loss: 0.1266 - val_accuracy: 0.9815\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1230 - accuracy: 0.9843 - val_loss: 0.1421 - val_accuracy: 0.9746\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9838 - val_loss: 0.1282 - val_accuracy: 0.9815\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1177 - accuracy: 0.9859 - val_loss: 0.1233 - val_accuracy: 0.9831\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1172 - accuracy: 0.9861 - val_loss: 0.1229 - val_accuracy: 0.9831\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1174 - accuracy: 0.9867 - val_loss: 0.1222 - val_accuracy: 0.9831\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1206 - accuracy: 0.9838 - val_loss: 0.1283 - val_accuracy: 0.9808\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1179 - accuracy: 0.9851 - val_loss: 0.1308 - val_accuracy: 0.9823\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1185 - accuracy: 0.9851 - val_loss: 0.1257 - val_accuracy: 0.9808\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1163 - accuracy: 0.9861 - val_loss: 0.1240 - val_accuracy: 0.9823\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1152 - accuracy: 0.9869 - val_loss: 0.1208 - val_accuracy: 0.9815\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1153 - accuracy: 0.9849 - val_loss: 0.1212 - val_accuracy: 0.9815\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1203 - accuracy: 0.9851 - val_loss: 0.1214 - val_accuracy: 0.9815\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1168 - accuracy: 0.9854 - val_loss: 0.1349 - val_accuracy: 0.9777\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1165 - accuracy: 0.9849 - val_loss: 0.1205 - val_accuracy: 0.9808\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1242 - accuracy: 0.9823 - val_loss: 0.1205 - val_accuracy: 0.9838\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1185 - accuracy: 0.9846 - val_loss: 0.1216 - val_accuracy: 0.9808\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1178 - accuracy: 0.9851 - val_loss: 0.1184 - val_accuracy: 0.9831\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1180 - accuracy: 0.9843 - val_loss: 0.1235 - val_accuracy: 0.9815\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1142 - accuracy: 0.9861 - val_loss: 0.1261 - val_accuracy: 0.9815\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1145 - accuracy: 0.9854 - val_loss: 0.1195 - val_accuracy: 0.9815\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1134 - accuracy: 0.9859 - val_loss: 0.1176 - val_accuracy: 0.9846\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1143 - accuracy: 0.9846 - val_loss: 0.1169 - val_accuracy: 0.9838\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1102 - accuracy: 0.9874 - val_loss: 0.1242 - val_accuracy: 0.9808\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1118 - accuracy: 0.9859 - val_loss: 0.1188 - val_accuracy: 0.9815\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9854 - val_loss: 0.1174 - val_accuracy: 0.9815\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1098 - accuracy: 0.9867 - val_loss: 0.1156 - val_accuracy: 0.9823\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1102 - accuracy: 0.9861 - val_loss: 0.1174 - val_accuracy: 0.9815\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1130 - accuracy: 0.9836 - val_loss: 0.1175 - val_accuracy: 0.9815\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1128 - accuracy: 0.9849 - val_loss: 0.1172 - val_accuracy: 0.9815\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1101 - accuracy: 0.9856 - val_loss: 0.1143 - val_accuracy: 0.9831\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1091 - accuracy: 0.9861 - val_loss: 0.1158 - val_accuracy: 0.9815\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1104 - accuracy: 0.9843 - val_loss: 0.1240 - val_accuracy: 0.9815\n",
      "Epoch 393/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1088 - accuracy: 0.9861 - val_loss: 0.1137 - val_accuracy: 0.9815\n",
      "Epoch 394/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1066 - accuracy: 0.9861 - val_loss: 0.1132 - val_accuracy: 0.9838\n",
      "Epoch 395/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1079 - accuracy: 0.9864 - val_loss: 0.1119 - val_accuracy: 0.9823\n",
      "Epoch 396/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1064 - accuracy: 0.9872 - val_loss: 0.1130 - val_accuracy: 0.9815\n",
      "Epoch 397/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1056 - accuracy: 0.9856 - val_loss: 0.1124 - val_accuracy: 0.9815\n",
      "Epoch 398/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1053 - accuracy: 0.9874 - val_loss: 0.1125 - val_accuracy: 0.9815\n",
      "Epoch 399/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1050 - accuracy: 0.9859 - val_loss: 0.1109 - val_accuracy: 0.9823\n",
      "Epoch 400/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1042 - accuracy: 0.9867 - val_loss: 0.1162 - val_accuracy: 0.9808\n",
      "Epoch 401/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1059 - accuracy: 0.9861 - val_loss: 0.1118 - val_accuracy: 0.9815\n",
      "Epoch 402/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1058 - accuracy: 0.9859 - val_loss: 0.1109 - val_accuracy: 0.9808\n",
      "Epoch 403/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1041 - accuracy: 0.9859 - val_loss: 0.1103 - val_accuracy: 0.9815\n",
      "Epoch 404/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1036 - accuracy: 0.9874 - val_loss: 0.1104 - val_accuracy: 0.9823\n",
      "Epoch 405/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1036 - accuracy: 0.9864 - val_loss: 0.1090 - val_accuracy: 0.9831\n",
      "Epoch 406/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1047 - accuracy: 0.9861 - val_loss: 0.1089 - val_accuracy: 0.9831\n",
      "Epoch 407/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1026 - accuracy: 0.9872 - val_loss: 0.1107 - val_accuracy: 0.9831\n",
      "Epoch 408/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1063 - accuracy: 0.9851 - val_loss: 0.1082 - val_accuracy: 0.9831\n",
      "Epoch 409/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1026 - accuracy: 0.9867 - val_loss: 0.1109 - val_accuracy: 0.9815\n",
      "Epoch 410/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1028 - accuracy: 0.9859 - val_loss: 0.1150 - val_accuracy: 0.9815\n",
      "Epoch 411/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1042 - accuracy: 0.9856 - val_loss: 0.1117 - val_accuracy: 0.9808\n",
      "Epoch 412/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1020 - accuracy: 0.9861 - val_loss: 0.1100 - val_accuracy: 0.9800\n",
      "Epoch 413/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1016 - accuracy: 0.9854 - val_loss: 0.1069 - val_accuracy: 0.9823\n",
      "Epoch 414/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1015 - accuracy: 0.9867 - val_loss: 0.1118 - val_accuracy: 0.9808\n",
      "Epoch 415/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1016 - accuracy: 0.9864 - val_loss: 0.1112 - val_accuracy: 0.9808\n",
      "Epoch 416/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0996 - accuracy: 0.9872 - val_loss: 0.1062 - val_accuracy: 0.9831\n",
      "Epoch 417/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1003 - accuracy: 0.9856 - val_loss: 0.1057 - val_accuracy: 0.9831\n",
      "Epoch 418/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1017 - accuracy: 0.9854 - val_loss: 0.1057 - val_accuracy: 0.9831\n",
      "Epoch 419/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0996 - accuracy: 0.9869 - val_loss: 0.1129 - val_accuracy: 0.9823\n",
      "Epoch 420/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1023 - accuracy: 0.9867 - val_loss: 0.1133 - val_accuracy: 0.9823\n",
      "Epoch 421/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1013 - accuracy: 0.9859 - val_loss: 0.1194 - val_accuracy: 0.9792\n",
      "Epoch 422/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1049 - accuracy: 0.9838 - val_loss: 0.1067 - val_accuracy: 0.9815\n",
      "Epoch 423/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1002 - accuracy: 0.9851 - val_loss: 0.1048 - val_accuracy: 0.9838\n",
      "Epoch 424/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0980 - accuracy: 0.9869 - val_loss: 0.1047 - val_accuracy: 0.9815\n",
      "Epoch 425/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0976 - accuracy: 0.9869 - val_loss: 0.1048 - val_accuracy: 0.9823\n",
      "Epoch 426/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0973 - accuracy: 0.9869 - val_loss: 0.1037 - val_accuracy: 0.9854\n",
      "Epoch 427/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0991 - accuracy: 0.9856 - val_loss: 0.1032 - val_accuracy: 0.9831\n",
      "Epoch 428/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0967 - accuracy: 0.9877 - val_loss: 0.1058 - val_accuracy: 0.9815\n",
      "Epoch 429/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0973 - accuracy: 0.9864 - val_loss: 0.1096 - val_accuracy: 0.9823\n",
      "Epoch 430/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0977 - accuracy: 0.9854 - val_loss: 0.1032 - val_accuracy: 0.9815\n",
      "Epoch 431/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0960 - accuracy: 0.9869 - val_loss: 0.1046 - val_accuracy: 0.9815\n",
      "Epoch 432/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0961 - accuracy: 0.9882 - val_loss: 0.1060 - val_accuracy: 0.9800\n",
      "Epoch 433/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0980 - accuracy: 0.9861 - val_loss: 0.1015 - val_accuracy: 0.9831\n",
      "Epoch 434/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0972 - accuracy: 0.9864 - val_loss: 0.1006 - val_accuracy: 0.9838\n",
      "Epoch 435/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0957 - accuracy: 0.9882 - val_loss: 0.1027 - val_accuracy: 0.9854\n",
      "Epoch 436/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0946 - accuracy: 0.9877 - val_loss: 0.1077 - val_accuracy: 0.9815\n",
      "Epoch 437/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0959 - accuracy: 0.9867 - val_loss: 0.1040 - val_accuracy: 0.9815\n",
      "Epoch 438/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0939 - accuracy: 0.9882 - val_loss: 0.1024 - val_accuracy: 0.9846\n",
      "Epoch 439/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0965 - accuracy: 0.9861 - val_loss: 0.1019 - val_accuracy: 0.9846\n",
      "Epoch 440/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0955 - accuracy: 0.9861 - val_loss: 0.0995 - val_accuracy: 0.9838\n",
      "Epoch 441/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0943 - accuracy: 0.9859 - val_loss: 0.0992 - val_accuracy: 0.9831\n",
      "Epoch 442/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0930 - accuracy: 0.9869 - val_loss: 0.1029 - val_accuracy: 0.9800\n",
      "Epoch 443/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0942 - accuracy: 0.9864 - val_loss: 0.1022 - val_accuracy: 0.9800\n",
      "Epoch 444/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0931 - accuracy: 0.9864 - val_loss: 0.0988 - val_accuracy: 0.9831\n",
      "Epoch 445/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0922 - accuracy: 0.9872 - val_loss: 0.0997 - val_accuracy: 0.9838\n",
      "Epoch 446/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0964 - accuracy: 0.9854 - val_loss: 0.0983 - val_accuracy: 0.9854\n",
      "Epoch 447/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0920 - accuracy: 0.9879 - val_loss: 0.0980 - val_accuracy: 0.9831\n",
      "Epoch 448/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0922 - accuracy: 0.9864 - val_loss: 0.0973 - val_accuracy: 0.9846\n",
      "Epoch 449/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0916 - accuracy: 0.9859 - val_loss: 0.0970 - val_accuracy: 0.9854\n",
      "Epoch 450/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0933 - accuracy: 0.9859 - val_loss: 0.0973 - val_accuracy: 0.9854\n",
      "Epoch 451/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0920 - accuracy: 0.9849 - val_loss: 0.0962 - val_accuracy: 0.9854\n",
      "Epoch 452/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0910 - accuracy: 0.9864 - val_loss: 0.0988 - val_accuracy: 0.9815\n",
      "Epoch 453/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0909 - accuracy: 0.9856 - val_loss: 0.0969 - val_accuracy: 0.9823\n",
      "Epoch 454/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0905 - accuracy: 0.9872 - val_loss: 0.0966 - val_accuracy: 0.9831\n",
      "Epoch 455/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0904 - accuracy: 0.9872 - val_loss: 0.0955 - val_accuracy: 0.9831\n",
      "Epoch 456/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0905 - accuracy: 0.9874 - val_loss: 0.0957 - val_accuracy: 0.9862\n",
      "Epoch 457/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0913 - accuracy: 0.9867 - val_loss: 0.0950 - val_accuracy: 0.9854\n",
      "Epoch 458/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0916 - accuracy: 0.9854 - val_loss: 0.0954 - val_accuracy: 0.9854\n",
      "Epoch 459/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0935 - accuracy: 0.9856 - val_loss: 0.1028 - val_accuracy: 0.9808\n",
      "Epoch 460/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0933 - accuracy: 0.9851 - val_loss: 0.0966 - val_accuracy: 0.9838\n",
      "Epoch 461/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0970 - accuracy: 0.9841 - val_loss: 0.0962 - val_accuracy: 0.9815\n",
      "Epoch 462/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0910 - accuracy: 0.9869 - val_loss: 0.1010 - val_accuracy: 0.9815\n",
      "Epoch 463/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0902 - accuracy: 0.9877 - val_loss: 0.0966 - val_accuracy: 0.9815\n",
      "Epoch 464/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0901 - accuracy: 0.9856 - val_loss: 0.1018 - val_accuracy: 0.9815\n",
      "Epoch 465/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0901 - accuracy: 0.9854 - val_loss: 0.0996 - val_accuracy: 0.9808\n",
      "Epoch 466/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0924 - accuracy: 0.9849 - val_loss: 0.1135 - val_accuracy: 0.9762\n",
      "Epoch 467/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0937 - accuracy: 0.9849 - val_loss: 0.0976 - val_accuracy: 0.9808\n",
      "Epoch 468/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0888 - accuracy: 0.9859 - val_loss: 0.0990 - val_accuracy: 0.9815\n",
      "Epoch 469/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0881 - accuracy: 0.9864 - val_loss: 0.0957 - val_accuracy: 0.9800\n",
      "Epoch 470/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0871 - accuracy: 0.9859 - val_loss: 0.0944 - val_accuracy: 0.9823\n",
      "Epoch 471/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0864 - accuracy: 0.9872 - val_loss: 0.0952 - val_accuracy: 0.9823\n",
      "Epoch 472/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0859 - accuracy: 0.9867 - val_loss: 0.0926 - val_accuracy: 0.9831\n",
      "Epoch 473/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0863 - accuracy: 0.9872 - val_loss: 0.0916 - val_accuracy: 0.9838\n",
      "Epoch 474/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0858 - accuracy: 0.9872 - val_loss: 0.0954 - val_accuracy: 0.9808\n",
      "Epoch 475/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0867 - accuracy: 0.9872 - val_loss: 0.0934 - val_accuracy: 0.9815\n",
      "Epoch 476/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0850 - accuracy: 0.9872 - val_loss: 0.0909 - val_accuracy: 0.9854\n",
      "Epoch 477/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0849 - accuracy: 0.9874 - val_loss: 0.0913 - val_accuracy: 0.9831\n",
      "Epoch 478/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0860 - accuracy: 0.9859 - val_loss: 0.0905 - val_accuracy: 0.9838\n",
      "Epoch 479/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0870 - accuracy: 0.9867 - val_loss: 0.0922 - val_accuracy: 0.9862\n",
      "Epoch 480/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0853 - accuracy: 0.9882 - val_loss: 0.0915 - val_accuracy: 0.9862\n",
      "Epoch 481/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0854 - accuracy: 0.9869 - val_loss: 0.0923 - val_accuracy: 0.9823\n",
      "Epoch 482/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0842 - accuracy: 0.9872 - val_loss: 0.0923 - val_accuracy: 0.9815\n",
      "Epoch 483/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0840 - accuracy: 0.9867 - val_loss: 0.0982 - val_accuracy: 0.9815\n",
      "Epoch 484/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0908 - accuracy: 0.9843 - val_loss: 0.0963 - val_accuracy: 0.9808\n",
      "Epoch 485/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0905 - accuracy: 0.9856 - val_loss: 0.0893 - val_accuracy: 0.9846\n",
      "Epoch 486/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0863 - accuracy: 0.9867 - val_loss: 0.0894 - val_accuracy: 0.9846\n",
      "Epoch 487/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0836 - accuracy: 0.9874 - val_loss: 0.0894 - val_accuracy: 0.9831\n",
      "Epoch 488/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0828 - accuracy: 0.9864 - val_loss: 0.0888 - val_accuracy: 0.9846\n",
      "Epoch 489/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0838 - accuracy: 0.9872 - val_loss: 0.0902 - val_accuracy: 0.9846\n",
      "Epoch 490/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0862 - accuracy: 0.9856 - val_loss: 0.0900 - val_accuracy: 0.9846\n",
      "Epoch 491/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0855 - accuracy: 0.9872 - val_loss: 0.0880 - val_accuracy: 0.9846\n",
      "Epoch 492/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0845 - accuracy: 0.9877 - val_loss: 0.0904 - val_accuracy: 0.9823\n",
      "Epoch 493/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0832 - accuracy: 0.9859 - val_loss: 0.0890 - val_accuracy: 0.9838\n",
      "Epoch 494/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0814 - accuracy: 0.9877 - val_loss: 0.0878 - val_accuracy: 0.9838\n",
      "Epoch 495/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0814 - accuracy: 0.9872 - val_loss: 0.0882 - val_accuracy: 0.9854\n",
      "Epoch 496/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0840 - accuracy: 0.9864 - val_loss: 0.0869 - val_accuracy: 0.9846\n",
      "Epoch 497/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0821 - accuracy: 0.9867 - val_loss: 0.0868 - val_accuracy: 0.9831\n",
      "Epoch 498/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0820 - accuracy: 0.9861 - val_loss: 0.0940 - val_accuracy: 0.9823\n",
      "Epoch 499/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0848 - accuracy: 0.9859 - val_loss: 0.0906 - val_accuracy: 0.9800\n",
      "Epoch 500/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0820 - accuracy: 0.9856 - val_loss: 0.0889 - val_accuracy: 0.9815\n",
      "Epoch 501/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0814 - accuracy: 0.9856 - val_loss: 0.0860 - val_accuracy: 0.9838\n",
      "Epoch 502/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0804 - accuracy: 0.9869 - val_loss: 0.0867 - val_accuracy: 0.9831\n",
      "Epoch 503/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0808 - accuracy: 0.9874 - val_loss: 0.0868 - val_accuracy: 0.9838\n",
      "Epoch 504/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0797 - accuracy: 0.9882 - val_loss: 0.0865 - val_accuracy: 0.9846\n",
      "Epoch 505/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0804 - accuracy: 0.9874 - val_loss: 0.0871 - val_accuracy: 0.9831\n",
      "Epoch 506/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0794 - accuracy: 0.9874 - val_loss: 0.0862 - val_accuracy: 0.9862\n",
      "Epoch 507/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0789 - accuracy: 0.9879 - val_loss: 0.0876 - val_accuracy: 0.9815\n",
      "Epoch 508/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0790 - accuracy: 0.9867 - val_loss: 0.0856 - val_accuracy: 0.9846\n",
      "Epoch 509/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0787 - accuracy: 0.9874 - val_loss: 0.0875 - val_accuracy: 0.9815\n",
      "Epoch 510/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0789 - accuracy: 0.9869 - val_loss: 0.0885 - val_accuracy: 0.9800\n",
      "Epoch 511/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0811 - accuracy: 0.9874 - val_loss: 0.0846 - val_accuracy: 0.9838\n",
      "Epoch 512/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0809 - accuracy: 0.9864 - val_loss: 0.0838 - val_accuracy: 0.9854\n",
      "Epoch 513/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9879 - val_loss: 0.0838 - val_accuracy: 0.9846\n",
      "Epoch 514/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0798 - accuracy: 0.9869 - val_loss: 0.0838 - val_accuracy: 0.9838\n",
      "Epoch 515/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9869 - val_loss: 0.0858 - val_accuracy: 0.9823\n",
      "Epoch 516/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0789 - accuracy: 0.9867 - val_loss: 0.0871 - val_accuracy: 0.9800\n",
      "Epoch 517/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0813 - accuracy: 0.9846 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
      "Epoch 518/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0824 - accuracy: 0.9864 - val_loss: 0.0956 - val_accuracy: 0.9808\n",
      "Epoch 519/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0819 - accuracy: 0.9846 - val_loss: 0.0884 - val_accuracy: 0.9815\n",
      "Epoch 520/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0842 - accuracy: 0.9854 - val_loss: 0.0829 - val_accuracy: 0.9846\n",
      "Epoch 521/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0792 - accuracy: 0.9859 - val_loss: 0.0845 - val_accuracy: 0.9854\n",
      "Epoch 522/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0807 - accuracy: 0.9854 - val_loss: 0.0822 - val_accuracy: 0.9846\n",
      "Epoch 523/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0773 - accuracy: 0.9872 - val_loss: 0.0837 - val_accuracy: 0.9831\n",
      "Epoch 524/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0751 - accuracy: 0.9864 - val_loss: 0.0812 - val_accuracy: 0.9846\n",
      "Epoch 525/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0753 - accuracy: 0.9869 - val_loss: 0.0837 - val_accuracy: 0.9838\n",
      "Epoch 526/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0747 - accuracy: 0.9874 - val_loss: 0.0813 - val_accuracy: 0.9846\n",
      "Epoch 527/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0746 - accuracy: 0.9861 - val_loss: 0.0804 - val_accuracy: 0.9846\n",
      "Epoch 528/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0747 - accuracy: 0.9890 - val_loss: 0.0807 - val_accuracy: 0.9838\n",
      "Epoch 529/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0744 - accuracy: 0.9859 - val_loss: 0.0799 - val_accuracy: 0.9838\n",
      "Epoch 530/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0745 - accuracy: 0.9869 - val_loss: 0.0795 - val_accuracy: 0.9846\n",
      "Epoch 531/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0747 - accuracy: 0.9867 - val_loss: 0.0850 - val_accuracy: 0.9808\n",
      "Epoch 532/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0791 - accuracy: 0.9867 - val_loss: 0.0817 - val_accuracy: 0.9823\n",
      "Epoch 533/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0767 - accuracy: 0.9877 - val_loss: 0.0792 - val_accuracy: 0.9846\n",
      "Epoch 534/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0752 - accuracy: 0.9864 - val_loss: 0.0789 - val_accuracy: 0.9846\n",
      "Epoch 535/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0750 - accuracy: 0.9874 - val_loss: 0.0792 - val_accuracy: 0.9846\n",
      "Epoch 536/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0749 - accuracy: 0.9861 - val_loss: 0.0811 - val_accuracy: 0.9838\n",
      "Epoch 537/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0731 - accuracy: 0.9872 - val_loss: 0.0812 - val_accuracy: 0.9831\n",
      "Epoch 538/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0733 - accuracy: 0.9874 - val_loss: 0.0795 - val_accuracy: 0.9838\n",
      "Epoch 539/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0734 - accuracy: 0.9864 - val_loss: 0.0835 - val_accuracy: 0.9815\n",
      "Epoch 540/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0738 - accuracy: 0.9887 - val_loss: 0.0784 - val_accuracy: 0.9838\n",
      "Epoch 541/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0720 - accuracy: 0.9861 - val_loss: 0.0780 - val_accuracy: 0.9862\n",
      "Epoch 542/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0727 - accuracy: 0.9869 - val_loss: 0.0780 - val_accuracy: 0.9854\n",
      "Epoch 543/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0723 - accuracy: 0.9869 - val_loss: 0.0795 - val_accuracy: 0.9838\n",
      "Epoch 544/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0741 - accuracy: 0.9864 - val_loss: 0.0834 - val_accuracy: 0.9831\n",
      "Epoch 545/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0749 - accuracy: 0.9861 - val_loss: 0.0824 - val_accuracy: 0.9823\n",
      "Epoch 546/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0729 - accuracy: 0.9874 - val_loss: 0.0845 - val_accuracy: 0.9823\n",
      "Epoch 547/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0718 - accuracy: 0.9887 - val_loss: 0.0799 - val_accuracy: 0.9823\n",
      "Epoch 548/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0720 - accuracy: 0.9872 - val_loss: 0.0887 - val_accuracy: 0.9815\n",
      "Epoch 549/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0746 - accuracy: 0.9849 - val_loss: 0.0858 - val_accuracy: 0.9815\n",
      "Epoch 550/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0755 - accuracy: 0.9869 - val_loss: 0.0972 - val_accuracy: 0.9792\n",
      "Epoch 551/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0761 - accuracy: 0.9859 - val_loss: 0.0777 - val_accuracy: 0.9846\n",
      "Epoch 552/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0720 - accuracy: 0.9879 - val_loss: 0.0773 - val_accuracy: 0.9854\n",
      "Epoch 553/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0708 - accuracy: 0.9885 - val_loss: 0.0758 - val_accuracy: 0.9854\n",
      "Epoch 554/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0698 - accuracy: 0.9877 - val_loss: 0.0771 - val_accuracy: 0.9846\n",
      "Epoch 555/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0700 - accuracy: 0.9874 - val_loss: 0.0769 - val_accuracy: 0.9838\n",
      "Epoch 556/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0700 - accuracy: 0.9877 - val_loss: 0.0772 - val_accuracy: 0.9831\n",
      "Epoch 557/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0699 - accuracy: 0.9869 - val_loss: 0.0772 - val_accuracy: 0.9846\n",
      "Epoch 558/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0696 - accuracy: 0.9864 - val_loss: 0.0751 - val_accuracy: 0.9846\n",
      "Epoch 559/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0693 - accuracy: 0.9867 - val_loss: 0.0782 - val_accuracy: 0.9823\n",
      "Epoch 560/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0711 - accuracy: 0.9879 - val_loss: 0.0801 - val_accuracy: 0.9823\n",
      "Epoch 561/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0707 - accuracy: 0.9874 - val_loss: 0.0901 - val_accuracy: 0.9792\n",
      "Epoch 562/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0757 - accuracy: 0.9859 - val_loss: 0.0826 - val_accuracy: 0.9815\n",
      "Epoch 563/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9859 - val_loss: 0.0781 - val_accuracy: 0.9831\n",
      "Epoch 564/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0678 - accuracy: 0.9887 - val_loss: 0.0744 - val_accuracy: 0.9854\n",
      "Epoch 565/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0706 - accuracy: 0.9864 - val_loss: 0.0738 - val_accuracy: 0.9838\n",
      "Epoch 566/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0691 - accuracy: 0.9879 - val_loss: 0.0750 - val_accuracy: 0.9854\n",
      "Epoch 567/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0726 - accuracy: 0.9867 - val_loss: 0.0732 - val_accuracy: 0.9854\n",
      "Epoch 568/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0758 - accuracy: 0.9859 - val_loss: 0.0740 - val_accuracy: 0.9838\n",
      "Epoch 569/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0696 - accuracy: 0.9869 - val_loss: 0.0738 - val_accuracy: 0.9846\n",
      "Epoch 570/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0692 - accuracy: 0.9874 - val_loss: 0.0740 - val_accuracy: 0.9862\n",
      "Epoch 571/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0734 - accuracy: 0.9864 - val_loss: 0.0728 - val_accuracy: 0.9838\n",
      "Epoch 572/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0675 - accuracy: 0.9882 - val_loss: 0.0741 - val_accuracy: 0.9854\n",
      "Epoch 573/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0668 - accuracy: 0.9869 - val_loss: 0.0731 - val_accuracy: 0.9846\n",
      "Epoch 574/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0666 - accuracy: 0.9885 - val_loss: 0.0759 - val_accuracy: 0.9823\n",
      "Epoch 575/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0676 - accuracy: 0.9879 - val_loss: 0.0829 - val_accuracy: 0.9831\n",
      "Epoch 576/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0699 - accuracy: 0.9859 - val_loss: 0.0774 - val_accuracy: 0.9815\n",
      "Epoch 577/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0664 - accuracy: 0.9867 - val_loss: 0.0719 - val_accuracy: 0.9854\n",
      "Epoch 578/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0669 - accuracy: 0.9872 - val_loss: 0.0726 - val_accuracy: 0.9854\n",
      "Epoch 579/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0666 - accuracy: 0.9895 - val_loss: 0.0714 - val_accuracy: 0.9846\n",
      "Epoch 580/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0657 - accuracy: 0.9879 - val_loss: 0.0724 - val_accuracy: 0.9862\n",
      "Epoch 581/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0696 - accuracy: 0.9874 - val_loss: 0.0711 - val_accuracy: 0.9854\n",
      "Epoch 582/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0662 - accuracy: 0.9882 - val_loss: 0.0709 - val_accuracy: 0.9854\n",
      "Epoch 583/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0658 - accuracy: 0.9882 - val_loss: 0.0728 - val_accuracy: 0.9862\n",
      "Epoch 584/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0718 - accuracy: 0.9867 - val_loss: 0.0723 - val_accuracy: 0.9854\n",
      "Epoch 585/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0688 - accuracy: 0.9864 - val_loss: 0.0713 - val_accuracy: 0.9846\n",
      "Epoch 586/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0688 - accuracy: 0.9864 - val_loss: 0.0795 - val_accuracy: 0.9823\n",
      "Epoch 587/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0683 - accuracy: 0.9882 - val_loss: 0.0758 - val_accuracy: 0.9800\n",
      "Epoch 588/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0656 - accuracy: 0.9890 - val_loss: 0.0730 - val_accuracy: 0.9846\n",
      "Epoch 589/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0644 - accuracy: 0.9874 - val_loss: 0.0716 - val_accuracy: 0.9838\n",
      "Epoch 590/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0653 - accuracy: 0.9877 - val_loss: 0.0736 - val_accuracy: 0.9831\n",
      "Epoch 591/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0661 - accuracy: 0.9874 - val_loss: 0.0797 - val_accuracy: 0.9831\n",
      "Epoch 592/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0651 - accuracy: 0.9872 - val_loss: 0.0713 - val_accuracy: 0.9831\n",
      "Epoch 593/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0651 - accuracy: 0.9895 - val_loss: 0.0886 - val_accuracy: 0.9792\n",
      "Epoch 594/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.9869 - val_loss: 0.0844 - val_accuracy: 0.9831\n",
      "Epoch 595/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0676 - accuracy: 0.9874 - val_loss: 0.0723 - val_accuracy: 0.9823\n",
      "Epoch 596/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0636 - accuracy: 0.9874 - val_loss: 0.0694 - val_accuracy: 0.9846\n",
      "Epoch 597/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0630 - accuracy: 0.9879 - val_loss: 0.0707 - val_accuracy: 0.9846\n",
      "Epoch 598/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0629 - accuracy: 0.9887 - val_loss: 0.0759 - val_accuracy: 0.9831\n",
      "Epoch 599/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0636 - accuracy: 0.9885 - val_loss: 0.0699 - val_accuracy: 0.9838\n",
      "Epoch 600/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0637 - accuracy: 0.9882 - val_loss: 0.0699 - val_accuracy: 0.9846\n",
      "Epoch 601/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0634 - accuracy: 0.9882 - val_loss: 0.0685 - val_accuracy: 0.9846\n",
      "Epoch 602/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0640 - accuracy: 0.9874 - val_loss: 0.0684 - val_accuracy: 0.9846\n",
      "Epoch 603/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0663 - accuracy: 0.9867 - val_loss: 0.0693 - val_accuracy: 0.9831\n",
      "Epoch 604/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0622 - accuracy: 0.9882 - val_loss: 0.0675 - val_accuracy: 0.9846\n",
      "Epoch 605/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0623 - accuracy: 0.9879 - val_loss: 0.0681 - val_accuracy: 0.9838\n",
      "Epoch 606/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0622 - accuracy: 0.9882 - val_loss: 0.0718 - val_accuracy: 0.9823\n",
      "Epoch 607/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0627 - accuracy: 0.9890 - val_loss: 0.0709 - val_accuracy: 0.9862\n",
      "Epoch 608/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0636 - accuracy: 0.9882 - val_loss: 0.0675 - val_accuracy: 0.9869\n",
      "Epoch 609/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0628 - accuracy: 0.9874 - val_loss: 0.0676 - val_accuracy: 0.9862\n",
      "Epoch 610/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0626 - accuracy: 0.9890 - val_loss: 0.0682 - val_accuracy: 0.9838\n",
      "Epoch 611/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0628 - accuracy: 0.9874 - val_loss: 0.0676 - val_accuracy: 0.9846\n",
      "Epoch 612/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0620 - accuracy: 0.9885 - val_loss: 0.0671 - val_accuracy: 0.9838\n",
      "Epoch 613/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0628 - accuracy: 0.9879 - val_loss: 0.0663 - val_accuracy: 0.9854\n",
      "Epoch 614/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0646 - accuracy: 0.9869 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
      "Epoch 615/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0632 - accuracy: 0.9892 - val_loss: 0.0663 - val_accuracy: 0.9846\n",
      "Epoch 616/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0629 - accuracy: 0.9879 - val_loss: 0.0678 - val_accuracy: 0.9838\n",
      "Epoch 617/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0607 - accuracy: 0.9877 - val_loss: 0.0671 - val_accuracy: 0.9838\n",
      "Epoch 618/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0606 - accuracy: 0.9882 - val_loss: 0.0689 - val_accuracy: 0.9831\n",
      "Epoch 619/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0603 - accuracy: 0.9890 - val_loss: 0.0663 - val_accuracy: 0.9846\n",
      "Epoch 620/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0630 - accuracy: 0.9879 - val_loss: 0.0681 - val_accuracy: 0.9815\n",
      "Epoch 621/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0627 - accuracy: 0.9869 - val_loss: 0.0682 - val_accuracy: 0.9862\n",
      "Epoch 622/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0649 - accuracy: 0.9879 - val_loss: 0.0739 - val_accuracy: 0.9808\n",
      "Epoch 623/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0621 - accuracy: 0.9882 - val_loss: 0.0675 - val_accuracy: 0.9823\n",
      "Epoch 624/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0613 - accuracy: 0.9879 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
      "Epoch 625/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0624 - accuracy: 0.9887 - val_loss: 0.0715 - val_accuracy: 0.9815\n",
      "Epoch 626/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0611 - accuracy: 0.9877 - val_loss: 0.0641 - val_accuracy: 0.9846\n",
      "Epoch 627/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0655 - accuracy: 0.9877 - val_loss: 0.0657 - val_accuracy: 0.9838\n",
      "Epoch 628/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0658 - accuracy: 0.9867 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
      "Epoch 629/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0677 - accuracy: 0.9864 - val_loss: 0.0686 - val_accuracy: 0.9815\n",
      "Epoch 630/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0626 - accuracy: 0.9872 - val_loss: 0.0700 - val_accuracy: 0.9815\n",
      "Epoch 631/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0604 - accuracy: 0.9877 - val_loss: 0.0735 - val_accuracy: 0.9831\n",
      "Epoch 632/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0631 - accuracy: 0.9867 - val_loss: 0.0695 - val_accuracy: 0.9823\n",
      "Epoch 633/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0590 - accuracy: 0.9885 - val_loss: 0.0651 - val_accuracy: 0.9854\n",
      "Epoch 634/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0594 - accuracy: 0.9874 - val_loss: 0.0640 - val_accuracy: 0.9854\n",
      "Epoch 635/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0653 - accuracy: 0.9879 - val_loss: 0.0661 - val_accuracy: 0.9838\n",
      "Epoch 636/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0636 - accuracy: 0.9872 - val_loss: 0.0678 - val_accuracy: 0.9815\n",
      "Epoch 637/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0600 - accuracy: 0.9872 - val_loss: 0.0686 - val_accuracy: 0.9823\n",
      "Epoch 638/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0607 - accuracy: 0.9882 - val_loss: 0.0881 - val_accuracy: 0.9777\n",
      "Epoch 639/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0651 - accuracy: 0.9851 - val_loss: 0.0769 - val_accuracy: 0.9831\n",
      "Epoch 640/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0629 - accuracy: 0.9882 - val_loss: 0.0750 - val_accuracy: 0.9823\n",
      "Epoch 641/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0602 - accuracy: 0.9869 - val_loss: 0.0683 - val_accuracy: 0.9815\n",
      "Epoch 642/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0586 - accuracy: 0.9882 - val_loss: 0.0661 - val_accuracy: 0.9838\n",
      "Epoch 643/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0599 - accuracy: 0.9877 - val_loss: 0.0683 - val_accuracy: 0.9815\n",
      "Epoch 644/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0613 - accuracy: 0.9890 - val_loss: 0.0685 - val_accuracy: 0.9838\n",
      "Epoch 645/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0583 - accuracy: 0.9890 - val_loss: 0.0627 - val_accuracy: 0.9846\n",
      "Epoch 646/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0594 - accuracy: 0.9872 - val_loss: 0.0680 - val_accuracy: 0.9815\n",
      "Epoch 647/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0592 - accuracy: 0.9877 - val_loss: 0.0646 - val_accuracy: 0.9846\n",
      "Epoch 648/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0611 - accuracy: 0.9867 - val_loss: 0.0665 - val_accuracy: 0.9846\n",
      "Epoch 649/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0656 - accuracy: 0.9854 - val_loss: 0.0653 - val_accuracy: 0.9869\n",
      "Epoch 650/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0596 - accuracy: 0.9879 - val_loss: 0.0629 - val_accuracy: 0.9862\n",
      "Epoch 651/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0604 - accuracy: 0.9882 - val_loss: 0.0693 - val_accuracy: 0.9831\n",
      "Epoch 652/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0574 - accuracy: 0.9887 - val_loss: 0.0720 - val_accuracy: 0.9823\n",
      "Epoch 653/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0600 - accuracy: 0.9867 - val_loss: 0.0793 - val_accuracy: 0.9823\n",
      "Epoch 654/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0656 - accuracy: 0.9859 - val_loss: 0.0852 - val_accuracy: 0.9792\n",
      "Epoch 655/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0641 - accuracy: 0.9867 - val_loss: 0.0624 - val_accuracy: 0.9846\n",
      "Epoch 656/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0576 - accuracy: 0.9879 - val_loss: 0.0691 - val_accuracy: 0.9815\n",
      "Epoch 657/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0621 - accuracy: 0.9867 - val_loss: 0.0615 - val_accuracy: 0.9854\n",
      "Epoch 658/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0574 - accuracy: 0.9887 - val_loss: 0.0640 - val_accuracy: 0.9838\n",
      "Epoch 659/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0558 - accuracy: 0.9885 - val_loss: 0.0617 - val_accuracy: 0.9846\n",
      "Epoch 660/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0587 - accuracy: 0.9882 - val_loss: 0.0614 - val_accuracy: 0.9854\n",
      "Epoch 661/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.9892 - val_loss: 0.0616 - val_accuracy: 0.9862\n",
      "Epoch 662/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0578 - accuracy: 0.9892 - val_loss: 0.0606 - val_accuracy: 0.9854\n",
      "Epoch 663/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0562 - accuracy: 0.9882 - val_loss: 0.0646 - val_accuracy: 0.9831\n",
      "Epoch 664/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0561 - accuracy: 0.9879 - val_loss: 0.0612 - val_accuracy: 0.9846\n",
      "Epoch 665/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0611 - accuracy: 0.9869 - val_loss: 0.0652 - val_accuracy: 0.9838\n",
      "Epoch 666/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0604 - accuracy: 0.9895 - val_loss: 0.0673 - val_accuracy: 0.9831\n",
      "Epoch 667/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0603 - accuracy: 0.9869 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
      "Epoch 668/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0619 - accuracy: 0.9864 - val_loss: 0.0663 - val_accuracy: 0.9823\n",
      "Epoch 669/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0603 - accuracy: 0.9867 - val_loss: 0.0624 - val_accuracy: 0.9846\n",
      "Epoch 670/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0574 - accuracy: 0.9874 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
      "Epoch 671/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0571 - accuracy: 0.9887 - val_loss: 0.0606 - val_accuracy: 0.9862\n",
      "Epoch 672/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0565 - accuracy: 0.9897 - val_loss: 0.0647 - val_accuracy: 0.9823\n",
      "Epoch 673/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0558 - accuracy: 0.9877 - val_loss: 0.0705 - val_accuracy: 0.9831\n",
      "Epoch 674/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0558 - accuracy: 0.9902 - val_loss: 0.0614 - val_accuracy: 0.9869\n",
      "Epoch 675/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0544 - accuracy: 0.9890 - val_loss: 0.0667 - val_accuracy: 0.9838\n",
      "Epoch 676/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0562 - accuracy: 0.9890 - val_loss: 0.0600 - val_accuracy: 0.9854\n",
      "Epoch 677/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9882 - val_loss: 0.0624 - val_accuracy: 0.9831\n",
      "Epoch 678/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0550 - accuracy: 0.9882 - val_loss: 0.0601 - val_accuracy: 0.9854\n",
      "Epoch 679/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0606 - accuracy: 0.9859 - val_loss: 0.0633 - val_accuracy: 0.9831\n",
      "Epoch 680/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0628 - accuracy: 0.9861 - val_loss: 0.0811 - val_accuracy: 0.9800\n",
      "Epoch 681/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0636 - accuracy: 0.9854 - val_loss: 0.0711 - val_accuracy: 0.9823\n",
      "Epoch 682/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0565 - accuracy: 0.9885 - val_loss: 0.0627 - val_accuracy: 0.9831\n",
      "Epoch 683/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0536 - accuracy: 0.9882 - val_loss: 0.0594 - val_accuracy: 0.9854\n",
      "Epoch 684/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0553 - accuracy: 0.9885 - val_loss: 0.0587 - val_accuracy: 0.9869\n",
      "Epoch 685/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0561 - accuracy: 0.9887 - val_loss: 0.0613 - val_accuracy: 0.9869\n",
      "Epoch 686/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0553 - accuracy: 0.9877 - val_loss: 0.0584 - val_accuracy: 0.9854\n",
      "Epoch 687/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0534 - accuracy: 0.9882 - val_loss: 0.0588 - val_accuracy: 0.9846\n",
      "Epoch 688/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9887 - val_loss: 0.0620 - val_accuracy: 0.9831\n",
      "Epoch 689/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9887 - val_loss: 0.0616 - val_accuracy: 0.9838\n",
      "Epoch 690/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9895 - val_loss: 0.0728 - val_accuracy: 0.9831\n",
      "Epoch 691/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0588 - accuracy: 0.9874 - val_loss: 0.0617 - val_accuracy: 0.9838\n",
      "Epoch 692/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9879 - val_loss: 0.0593 - val_accuracy: 0.9846\n",
      "Epoch 693/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0549 - accuracy: 0.9887 - val_loss: 0.0589 - val_accuracy: 0.9846\n",
      "Epoch 694/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0530 - accuracy: 0.9882 - val_loss: 0.0582 - val_accuracy: 0.9854\n",
      "Epoch 695/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0546 - accuracy: 0.9885 - val_loss: 0.0601 - val_accuracy: 0.9831\n",
      "Epoch 696/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0528 - accuracy: 0.9887 - val_loss: 0.0683 - val_accuracy: 0.9823\n",
      "Epoch 697/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0545 - accuracy: 0.9882 - val_loss: 0.0605 - val_accuracy: 0.9838\n",
      "Epoch 698/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0524 - accuracy: 0.9887 - val_loss: 0.0593 - val_accuracy: 0.9862\n",
      "Epoch 699/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0525 - accuracy: 0.9892 - val_loss: 0.0601 - val_accuracy: 0.9838\n",
      "Epoch 700/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9887 - val_loss: 0.0633 - val_accuracy: 0.9846\n",
      "Epoch 701/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0543 - accuracy: 0.9879 - val_loss: 0.0639 - val_accuracy: 0.9838\n",
      "Epoch 702/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0559 - accuracy: 0.9877 - val_loss: 0.0580 - val_accuracy: 0.9846\n",
      "Epoch 703/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0535 - accuracy: 0.9902 - val_loss: 0.0569 - val_accuracy: 0.9862\n",
      "Epoch 704/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0528 - accuracy: 0.9897 - val_loss: 0.0571 - val_accuracy: 0.9854\n",
      "Epoch 705/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0530 - accuracy: 0.9895 - val_loss: 0.0572 - val_accuracy: 0.9862\n",
      "Epoch 706/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0539 - accuracy: 0.9890 - val_loss: 0.0630 - val_accuracy: 0.9831\n",
      "Epoch 707/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0525 - accuracy: 0.9897 - val_loss: 0.0684 - val_accuracy: 0.9823\n",
      "Epoch 708/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0574 - accuracy: 0.9877 - val_loss: 0.0732 - val_accuracy: 0.9815\n",
      "Epoch 709/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0601 - accuracy: 0.9856 - val_loss: 0.0807 - val_accuracy: 0.9800\n",
      "Epoch 710/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0584 - accuracy: 0.9861 - val_loss: 0.0661 - val_accuracy: 0.9815\n",
      "Epoch 711/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0589 - accuracy: 0.9859 - val_loss: 0.0587 - val_accuracy: 0.9846\n",
      "Epoch 712/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0524 - accuracy: 0.9902 - val_loss: 0.0572 - val_accuracy: 0.9862\n",
      "Epoch 713/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0544 - accuracy: 0.9882 - val_loss: 0.0574 - val_accuracy: 0.9869\n",
      "Epoch 714/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0518 - accuracy: 0.9892 - val_loss: 0.0657 - val_accuracy: 0.9823\n",
      "Epoch 715/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0517 - accuracy: 0.9890 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
      "Epoch 716/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9892 - val_loss: 0.0566 - val_accuracy: 0.9854\n",
      "Epoch 717/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0519 - accuracy: 0.9885 - val_loss: 0.0561 - val_accuracy: 0.9854\n",
      "Epoch 718/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9897 - val_loss: 0.0597 - val_accuracy: 0.9846\n",
      "Epoch 719/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0516 - accuracy: 0.9885 - val_loss: 0.0600 - val_accuracy: 0.9831\n",
      "Epoch 720/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0519 - accuracy: 0.9885 - val_loss: 0.0570 - val_accuracy: 0.9862\n",
      "Epoch 721/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0514 - accuracy: 0.9869 - val_loss: 0.0560 - val_accuracy: 0.9854\n",
      "Epoch 722/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0518 - accuracy: 0.9890 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
      "Epoch 723/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0507 - accuracy: 0.9892 - val_loss: 0.0566 - val_accuracy: 0.9854\n",
      "Epoch 724/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9882 - val_loss: 0.0562 - val_accuracy: 0.9862\n",
      "Epoch 725/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0528 - accuracy: 0.9885 - val_loss: 0.0549 - val_accuracy: 0.9862\n",
      "Epoch 726/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0518 - accuracy: 0.9892 - val_loss: 0.0592 - val_accuracy: 0.9838\n",
      "Epoch 727/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0497 - accuracy: 0.9885 - val_loss: 0.0577 - val_accuracy: 0.9846\n",
      "Epoch 728/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0514 - accuracy: 0.9902 - val_loss: 0.0609 - val_accuracy: 0.9831\n",
      "Epoch 729/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0495 - accuracy: 0.9900 - val_loss: 0.0541 - val_accuracy: 0.9862\n",
      "Epoch 730/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0509 - accuracy: 0.9874 - val_loss: 0.0615 - val_accuracy: 0.9838\n",
      "Epoch 731/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0483 - accuracy: 0.9900 - val_loss: 0.0548 - val_accuracy: 0.9854\n",
      "Epoch 732/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0524 - accuracy: 0.9885 - val_loss: 0.0552 - val_accuracy: 0.9854\n",
      "Epoch 733/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0590 - accuracy: 0.9864 - val_loss: 0.0563 - val_accuracy: 0.9846\n",
      "Epoch 734/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0556 - accuracy: 0.9864 - val_loss: 0.0590 - val_accuracy: 0.9838\n",
      "Epoch 735/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0532 - accuracy: 0.9887 - val_loss: 0.0579 - val_accuracy: 0.9831\n",
      "Epoch 736/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0514 - accuracy: 0.9879 - val_loss: 0.0543 - val_accuracy: 0.9846\n",
      "Epoch 737/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0504 - accuracy: 0.9885 - val_loss: 0.0548 - val_accuracy: 0.9862\n",
      "Epoch 738/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0494 - accuracy: 0.9887 - val_loss: 0.0560 - val_accuracy: 0.9854\n",
      "Epoch 739/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0488 - accuracy: 0.9897 - val_loss: 0.0763 - val_accuracy: 0.9808\n",
      "Epoch 740/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0534 - accuracy: 0.9885 - val_loss: 0.0611 - val_accuracy: 0.9831\n",
      "Epoch 741/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0536 - accuracy: 0.9877 - val_loss: 0.0543 - val_accuracy: 0.9846\n",
      "Epoch 742/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0506 - accuracy: 0.9877 - val_loss: 0.0577 - val_accuracy: 0.9831\n",
      "Epoch 743/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0508 - accuracy: 0.9872 - val_loss: 0.0564 - val_accuracy: 0.9846\n",
      "Epoch 744/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0490 - accuracy: 0.9895 - val_loss: 0.0546 - val_accuracy: 0.9854\n",
      "Epoch 745/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0491 - accuracy: 0.9895 - val_loss: 0.0556 - val_accuracy: 0.9862\n",
      "Epoch 746/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0532 - accuracy: 0.9879 - val_loss: 0.0544 - val_accuracy: 0.9877\n",
      "Epoch 747/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0521 - accuracy: 0.9885 - val_loss: 0.0544 - val_accuracy: 0.9869\n",
      "Epoch 748/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0496 - accuracy: 0.9902 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
      "Epoch 749/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0504 - accuracy: 0.9890 - val_loss: 0.0681 - val_accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, \n",
    "                  callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 8ms/step - loss: 0.1070 - accuracy: 0.9815\n",
      "Test accuracy: 0.9815384745597839\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
